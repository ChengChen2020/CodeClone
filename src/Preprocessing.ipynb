{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "train_fold = 'train/'\n",
    "test_fold = 'test/'\n",
    "ext = '.txt'\n",
    "for filename in os.listdir(test_fold):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = test_fold + filename\n",
    "        ffile = 'ntest/' + filename\n",
    "        os.system('clang -cc1 -dump-tokens ' + path + ' 2> ' + ffile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "usle = set(['', '1', '_Bool', 'eof', 'r_pare', 'r_paren', 'r_brace', 'r_square', 'semi', \n",
    "            'train/0ae1/8c126b029441457e.txt:81:17:', 'train/e892/a1eb2cf347244c59.txt:22:33:'])\n",
    "feature = list(token - usle)\n",
    "feature\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store\n",
    "with open('./feature.pkl', 'wb') as f:\n",
    "    pickle.dump(feature,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "import pickle\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "feature = load_data('feature.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(path):\n",
    "    with open(path) as f:\n",
    "        docs = f.readlines()\n",
    "        for i in range(len(docs)):\n",
    "            p = docs[i].find('\\t')\n",
    "            docs[i] = docs[i][:p].split(' ')[0]\n",
    "        d = Counter(docs)\n",
    "        v = []\n",
    "        for i in feature:\n",
    "            v.append(d[i])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "train_fold = 'nntrain/'\n",
    "test_fold = 'test/'\n",
    "ext = '.txt'\n",
    "test = pd.read_csv('sample_submission.csv')\n",
    "predictions = []\n",
    "\n",
    "\n",
    "\n",
    "vvv = []\n",
    "token = set([])\n",
    "for clone_fold in os.listdir(train_fold):\n",
    "    if len(clone_fold) == 4:\n",
    "        clone = os.listdir(train_fold + clone_fold)\n",
    "        vv = []\n",
    "        for filename in clone:\n",
    "            if filename.endswith('.txt'):\n",
    "                path = train_fold + clone_fold + '/' + filename\n",
    "                vv.append(vectorize(path))\n",
    "        vvv.append(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vvv[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./TrainVec.pkl', 'wb') as f:\n",
    "    pickle.dump(vvv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "X = []\n",
    "y = []\n",
    "X_ = []\n",
    "y_ = []\n",
    "# train\n",
    "for i in range(83):\n",
    "    for j in range(2000):\n",
    "        a = random.randint(0,499)\n",
    "        b = random.randint(0,499)\n",
    "        if a != b:\n",
    "            v = vvv[i][a] + vvv[i][b]\n",
    "            X.append([v])\n",
    "            y.append(1)\n",
    "for i in range(83*20000):\n",
    "    a = random.randint(0,82)\n",
    "    b = random.randint(0,82)\n",
    "    if a != b:\n",
    "        z1 = random.randint(0,499)\n",
    "        z2 = random.randint(0,499)\n",
    "        v = vvv[a][z1] + vvv[b][z2]\n",
    "        X.append([v])\n",
    "        y.append(0)\n",
    "# valid\n",
    "for i in range(73, 83):\n",
    "    for j in range(1000):\n",
    "        a = random.randint(0,499)\n",
    "        b = random.randint(0,499)\n",
    "        if a != b:\n",
    "            v = vvv[i][a] + vvv[i][b]\n",
    "            X_.append([v])\n",
    "            y_.append(1)\n",
    "for i in range(10*10000):\n",
    "    a = random.randint(73,82)\n",
    "    b = random.randint(73,82)\n",
    "    if a != b:\n",
    "        z1 = random.randint(0,499)\n",
    "        z2 = random.randint(0,499)\n",
    "        v = vvv[a][z1] + vvv[b][z2]\n",
    "        X_.append([v])\n",
    "        y_.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(X).reshape(-1,2,73)\n",
    "y = np.array(y)\n",
    "X_ = np.array(X_).reshape(-1,2,73)\n",
    "y_ = np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# shuffle\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "X_, y_ = shuffle(X_, y_, random_state=0)\n",
    "X.shape, y.shape, X_.shape, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'X': X, 'y': y}\n",
    "d_ = {'X_': X_, 'y_': y_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store training set and validation set\n",
    "import pickle\n",
    "with open('./MyData1.pkl', 'wb') as f:\n",
    "    pickle.dump(d, f)\n",
    "with open('./MyData2.pkl', 'wb') as f:\n",
    "    pickle.dump(d_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pair = [i.split('_') for i in test.id1_id2]\n",
    "path_pair = [['ntest/' + a + ext, 'ntest/' + b + ext] for a,b in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_pair = [[vectorize(a), vectorize(b)] for a,b in path_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.array([np.array(i).reshape(1,2,73) for i in vec_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./TestVec.pkl', 'wb') as f:\n",
    "    pickle.dump(vec,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
